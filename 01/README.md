# Методы оптимизации: Градиентный спуск и метод Ньютона

Этот ноутбук содержит реализацию двух классических численных методов оптимизации:

- **Градиентный спуск** для квадратичной функции
- **Метод Ньютона** с адаптивным шагом и проверкой положительной определённости гессиана


---

##  Цель

Продемонстрировать работу методов оптимизации на примере двумерных квадратичных функций:
- `f(x₁, x₂) = x₁² + x₂²`
- `f(x₁, x₂) = x₁² + 2x₂²`

Для каждой функции реализованы:
- градиент и гессиан,
- алгоритмы оптимизации,
- визуализация сходимости.

---

## Реализовано

### Градиентный спуск

- Функция: `f(x) = x₁² + x₂²`
- Градиент: `∇f = [2x₁, 2x₂]`
- Алгоритм с фиксированным шагом `α`
- Критерий остановки: `||∇f|| < threshold`
- Визуализация нормы градиента по итерациям

Результат: метод сошёлся к точке `[0, 0]` за 70 итераций

---

### Метод Ньютона

- Функция: `f(x) = x₁² + 2x₂²`
- Гессиан: `H = [[2, 0], [0, 4]]`
- Адаптивный шаг с уменьшением (`beta`)
- Проверка положительной определённости гессиана:
  если хотя бы одно собственное значение ≤ 0 — корректировка с помощью `H + γI`
- Критерий остановки: `||∇f|| < threshold`
- Визуализация значений функции на каждой итерации

Результат: метод сошёлся за **2 итерации** к точке `[0, 0]`  




```bash
git clone https://github.com/yourusername/yourrepo.git
cd yourrepo
jupyter notebook 01_newton_method.ipynb
