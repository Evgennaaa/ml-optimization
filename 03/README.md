# Сравнение методов оптимизации: Градиентный спуск и Метод Ньютона

Данный ноутбук содержит реализацию и сравнение двух популярных численных методов оптимизации на примере квадратичной функции двух переменных:

\[
f(x_1, x_2) = x_1^2 + 2x_2^2
\]

---

## Реализованные методы

### Градиентный спуск с адаптивным шагом
- Метод не использует вторые производные
- При каждом шаге шаг `α` уменьшается по правилу: `α = α * β`, если новое значение не уменьшает функцию
- Критерий остановки: `||∇f|| < threshold`
- Сходимость достигнута за **3 итерации**
- Траектория отображена на графике

### Метод Ньютона
- Использует градиент и гессиан функции
- Корректирует гессиан, если он не положительно определён (добавляет `γ * I`)
- Сходимость достигнута за **2 итерации**
- Отображает траекторию на графике

---

